{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f505073d-5867-4aad-b6e3-6c954107b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347b4f42-530a-4865-8d1e-4b3042bbf4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets transformers tokenizers pydantic auto_gptq gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c397948f-a0f8-47a8-947f-82a07829f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df433884-3944-46a5-8673-b81e999d2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "# adding UtilityFuncts to the system path\n",
    "sys.path.insert(0, '/notebooks/Utility')\n",
    "import UtilityFuncts as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5bf102-ccee-4b32-aa23-0325c4e1b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f700ce1-6918-4d8b-8da2-a1c635e2dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021d41d1-1530-4564-a2bf-7b2782288af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM into GPU memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/auto/tokenization_auto.py:648: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e816a632cf3a449d8a7238ed3ff0c61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Loading LLM into GPU memory\")\n",
    "# it is suggested to pin the revision commit hash and not change it for reproducibility \n",
    "# because the uploader might change the model afterwards; you can find the commmit history of \n",
    "# llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "token = \"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\", use_auth_token = token)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    revision=revision,\n",
    "    return_full_text=False,\n",
    "    # use_auth_token = token\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0930e0a-25c3-4f47-80c1-222ab354a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to generate text\n",
    "def gen_text(prompts, use_template=False, **kwargs):\n",
    "    if use_template:\n",
    "        full_prompts = [\n",
    "            PROMPT_FOR_GENERATION_FORMAT.format(advertisement=prompt)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        full_prompts = prompts\n",
    "\n",
    "    if \"batch_size\" not in kwargs:\n",
    "        kwargs[\"batch_size\"] = 1\n",
    "    \n",
    "    # the default max length is pretty small (20), which would cut the generated output in the middle, so it's necessary to increase the threshold to the complete response\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 512\n",
    "\n",
    "    # configure other text generation arguments, see common configurable args here: \n",
    "    #https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,  # Hugging Face sets pad_token_id to eos_token_id by default; setting here to not see redundant message\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "    # print(full_prompts)\n",
    "    outputs = pipeline(full_prompts, **kwargs)\n",
    "    \n",
    "    outputs = [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Generate work History from csv\n",
    "# Create work roles dataframe\n",
    "wr_path = \"/notebooks/Inputs/WorkHistory_Roles.csv\"\n",
    "wr_df = pd.read_csv(wr_path) \n",
    "\n",
    "# Create Work Achievements dataframe\n",
    "wa_path = \"/notebooks/Inputs/WorkHistory_Achievments.csv\"\n",
    "wa_df = pd.read_csv(wa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1bbfbbc-f20b-4fe6-b427-d5a0f1c8f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate work history list\n",
    "work_history_list = []\n",
    "\n",
    "for index, row in wr_df.iterrows():\n",
    "    # Iterate through the role dataframe\n",
    "    work_history_list.append(f\"Role: {row['Role']}\")\n",
    "    work_history_list.append(f\"Company: {row['Company']}\")\n",
    "    work_history_list.append(f\"Location: {row['Location']}\")\n",
    "    work_history_list.append(f\"Duration: {row['Period']}\")\n",
    "    work_history_list.append(\"Achievements:\")\n",
    "    _wa_df = wa_df[wa_df['role_id'] == row['role_id']]\n",
    "    \n",
    "    # Iterate through the achievements dataframe\n",
    "    for index_wa, row_wa in _wa_df.iterrows():\n",
    "        work_history_list.append(f\"- {row_wa['achievement']}\")\n",
    "    work_history_list.append(\"\")\n",
    "\n",
    "# Conver this to a string\n",
    "work_history = \"\\n\".join(work_history_list)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get job ads dictionary\n",
    "job_ads = uf.obtain_ad_folder_dict()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728af005-c35c-4511-95ab-ce298f74aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cover letter for ad1\n"
     ]
    }
   ],
   "source": [
    "# Run a loop to cover letters for each of the advertisements\n",
    "# for ad, folder in job_ads.items():\n",
    "    # Start timer\n",
    "print(f\"Generating cover letter for ad1\")\n",
    "\n",
    "job_ad_path = f\"/notebooks/Job Ads/New Ad/ad1.txt\"\n",
    "\n",
    "with open(job_ad_path, \"r\", encoding='utf-8') as f:\n",
    "    job_ad = f.read()\n",
    "    \n",
    "DEFAULT_SYSTEM_PROMPT = f\"\"\"\\\n",
    "You are an intelligent, respectful and honest job seeker. This is your work history: {work_history}. You are applying for a job. Always answer as professionally as possible, while being friendly. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. Below is an advertisment that describes a job you are interested in applying for. Write a short, succinct, professional cover letter in response to the job, making sure to highlight relevant details from your work history.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"\n",
    "<s>[INST]<<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{advertisement}\n",
    "[/INST]\n",
    "\"\"\".format(\n",
    "    system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "    advertisement=\"{advertisement}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775ee31a-3abe-46d4-8485-bc7f345ac8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "    # Generate the text\n",
    "results = gen_text([job_ad], use_template = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50c0f90d-763c-4004-b9ae-17416dca152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Dear Hiring Manager,\\n\\nI am writing to express my strong interest in the Data Scientist position at the NSW Department of Education. As a seasoned data scientist with a proven track record of delivering data-driven insights and solutions, I am confident that I can make a valuable contribution to your team.\\n\\nI am particularly impressed by the Department's commitment to leveraging data and insights to drive its vision, and I am excited about the opportunity to play a role in this effort. I am also drawn to the Department's focus on creating a culture of data-driven decision-making, and I believe that my experience and skills align well with this goal.\\n\\nIn my current role as a Data Science Manager at Company XYZ, I have led a team of data scientists and analysts in developing cutting-edge machine learning models for predictive maintenance, resulting in a 30% reduction in maintenance costs. I have also implemented a data-driven recommendation system that increased user engagement by 25%, and spearheaded the adoption of advanced NLP techniques for sentiment analysis, improving customer feedback analysis by 40%.\\n\\nIn my previous roles, I have consistently demonstrated my ability to communicate complex analytical concepts to non-technical stakeholders, and to translate data insights into actionable recommendations. I am confident that I can bring this same level of expertise and dedication to the Department of Education.\\n\\nIn my answer to the first pre-screening question, I would like to provide an example of how I have translated complex data analyses into practical recommendations. In my current role, I led a project to develop a predictive model for maintenance scheduling, which resulted in a significant reduction in maintenance costs. I presented my findings to the client's leadership team, and provided recommendations for how to implement the model in their operations.\\n\\nIn my answer to the second pre-screening question, I would like to provide an overview of a time when I, or a team I was a part of, built a model to solve a real-world problem. In my previous role as a Senior Data Scientist at Tech Innovations Inc., I worked on a project to develop a fraud detection system using anomaly detection algorithms. Our approach involved using a combination of supervised and unsupervised learning techniques to identify patterns in transactional data\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b178c997-9b51-4c04-b5b8-6962e2a243f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output\n",
    "output_path = f'/notebooks/Resume Components/ad1_2023-09-09_06-10-41/cover_letter_text.txt'\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(results[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
