{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097e9392-bec0-40da-99f2-60799273934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import transformers\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3c8a59-e9d9-4f52-b3b5-1827f3df9082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM into GPU memory\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Loading LLM into GPU memory\")\n",
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "token = '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' # Token from huggingface website - JG\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\", token = token)\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "#     return_full_text=False,\n",
    "#     use_auth_token = token\n",
    "# )\n",
    "\n",
    "# # Required tokenizer setting for batch inference\n",
    "# pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# end = time.time()\n",
    "\n",
    "# duration = end-start\n",
    "# print(f\"Load finished. Duration: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4542745-ba25-4b25-9f9a-038a36ed1a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To purge CUDA\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6186ad-3757-4ae8-96c4-83cedc8e30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to generate text\n",
    "def gen_text(prompts, use_template=False, **kwargs):\n",
    "    if use_template:\n",
    "        full_prompts = [\n",
    "            PROMPT_FOR_GENERATION_FORMAT.format(advertisement=prompt)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        full_prompts = prompts\n",
    "\n",
    "    if \"batch_size\" not in kwargs:\n",
    "        kwargs[\"batch_size\"] = 1\n",
    "    \n",
    "    # the default max length is pretty small (20), which would cut the generated output in the middle, so it's necessary to increase the threshold to the complete response\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 512\n",
    "\n",
    "    # configure other text generation arguments, see common configurable args here: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,  # Hugging Face sets pad_token_id to eos_token_id by default; setting here to not see redundant message\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "    # print(full_prompts)\n",
    "    outputs = pipeline(full_prompts, **kwargs)\n",
    "    \n",
    "    outputs = [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
